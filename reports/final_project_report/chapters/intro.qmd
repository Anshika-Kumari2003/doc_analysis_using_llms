# Introduction

With the exponential growth of unstructured data—ranging from documents and multimedia to tabular datasets—extracting meaningful insights through traditional rule-based systems has become a significant challenge. Recent advances in **Large Language Models (LLMs)** have enabled more intelligent, context-aware, and human-like interaction with data, marking a paradigm shift in the field of natural language processing (NLP) and AI-powered information retrieval [@brown2020language].

This project introduces a **multi-modal AI system** designed to handle different data types through three intelligent modules:

1. **Multi-Agent Assistant Module:**
   In addition to handling structured, semi-structured, and unstructured data, this project incorporates a Multi-Agent Assistant designed to intelligently route user queries across multiple specialized agents. Leveraging lightweight LLMs like Phi-3 Mini deployed via Ollama, the assistant dynamically selects appropriate tools to handle company-specific financial queries (EnerSys, Apple, NVIDIA), YouTube video search, or general web-based information retrieval. The system employs a routing mechanism where the LLM acts as a controller to select between Retrieval-Augmented Generation (RAG), API-based search agents, and real-time web queries, ensuring accurate and context-sensitive responses [@liu2023agentbench]. This multi-agent design showcases how LLMs can be orchestrated to serve as versatile AI-powered knowledge assistants across diverse information domains.

2. **Document Analysis via RAG (Retrieval-Augmented Generation):**  
   This module allows users to upload PDF or TXT documents, which are semantically chunked and embedded into a vector database. Leveraging **Phi-3 Mini**, a lightweight LLM, users can ask context-sensitive questions and receive accurate answers based on retrieved document chunks [@lewis2020rag].

3. **YouTube Transcript Agent:**  
   Using state-of-the-art speech recognition models like **Youtube Transcipt API** [@radford2023whisper], this module transcribes YouTube videos, summarizes the content, and answers follow-up questions through a GUI-powered interface. It offers a valuable solution for educational or research-based video analysis [@rao2023yt].

4. **SQL Agent for Structured Data (Excel):**  
   Users can upload Excel sheets containing structured data, and the system converts natural language questions into executable SQL queries. This allows for intuitive querying without needing manual database interaction, drawing on advances in text-to-SQL modeling [@yu2018spider; @shi2022text2sql].

By combining document retrieval, audio processing, and structured data querying, this system demonstrates how modern LLMs can bridge the gap between raw data and human-centric insights across domains.


## Overview

In today’s information-rich world, users often struggle to extract insights from documents, videos, or spreadsheets without specialized tools or technical skills. This project bridges that gap through an AI-powered system that allows users to “talk” to their data—regardless of its format.

The system is built around three intelligent agents:

- A **A Multi-Agent Assistant** that automatically analyzes user queries and routes them to the most suitable agent—whether it involves company-specific data, video searches, or web-based information retrieval—creating a unified conversational interface.
- A **Document Agent** that reads and answers questions from uploaded PDFs or text files using retrieval-augmented generation.
- A **YouTube Agent** that listens to videos, summarizes key points, and provides answers based on transcripts.
- A **SQL Agent** that converts plain English queries into SQL to interpret Excel sheets effortlessly.

By combining the strengths of language models, semantic search, and speech recognition, this solution offers a unified, conversational way to analyze diverse types of content—no coding required.


## Objectives of Project

The primary goal of this project is to develop an intelligent, multi-modal AI system capable of understanding and interacting with diverse types of data using natural language. The specific objectives are:

1. **Enhance Information Retrieval from Documents:**
   - Implement a Retrieval-Augmented Generation (RAG) pipeline to answer user queries based on uploaded PDFs or text files.
   - Use semantic chunking and vector embeddings to ensure relevant and accurate responses.

2. **Simplify Video Content Understanding:**
   - Use Whisper to transcribe audio from YouTube videos.
   - Generate summaries and enable question-answering on the transcribed content to help users extract key insights quickly.

3. **Enable Natural Language Interaction with Tabular Data:**
   - Accept Excel file uploads and parse structured data.
   - Convert user questions into SQL queries using text-to-SQL methods, and display results clearly.

4. **Provide a Unified, User-Friendly Interface:**
   - Design modular and accessible interfaces (e.g., via Gradio or Jupyter Widgets) to support smooth interaction with all three modules.

5. **Demonstrate Real-World Applicability of LLMs:**
   - Showcase how lightweight language models (like Phi-3 Mini) can handle diverse tasks across domains with minimal user effort.


 



















