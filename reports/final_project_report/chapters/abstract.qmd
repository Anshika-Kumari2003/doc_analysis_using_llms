# Abstract {.unnumbered}

This project presents a comprehensive multi module AI system that enables intelligent interaction with diverse unstructured and structured data sources using Large Language Models (LLMs).

The first module introduces a Multi-Agent Assistant that acts as a dynamic controller. It analyzes user queries and conversation history to intelligently select the appropriate module—whether document RAG, YouTube transcription, SQL querying, or external web search—enabling seamless, unified access to multiple information sources through a single conversational interface.

The second module implements a Retrieval-Augmented Generation (RAG) pipeline for document analysis. Users upload PDF or TXT files, which are chunked, embedded using transformer-based models, and stored in a vector database. A lightweight local LLM (Phi-3 Mini) then powers context-aware semantic search and question answering via a Gradio interface.

The third module is a YouTube Transcript Agent. Users input video URLs, and transcripts are either extracted from captions or generated using models like Youtube Transcript Model. The transcripts are then processed for summarization and question answering using LangChain and Phi-3, accessible through a user-friendly Jupyter notebook interface.

The fourth module functions as an SQL Agent for tabular data, where users can upload Excel sheets. These files are parsed, and natural language queries are translated into SQL to fetch insights directly from the data, offering a conversational way to explore spreadsheets.

Collectively, these modules showcase the power of modern NLP techniques to convert static documents, audio-visual content, and structured tables into interactive, insightful experiences, with applications in education, business intelligence, and research.
